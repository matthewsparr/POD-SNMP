{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpeJSO8uKw_O"
      },
      "source": [
        "## import"
      ],
      "id": "fpeJSO8uKw_O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "624654d8-aed4-418b-9506-c2d48f36a6c9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install \"ray[rllib]\" --quiet\n",
        "!pip install gym --quiet\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from uuid import uuid4 as uuid\n",
        "import sys\n",
        "import copy\n",
        "import os\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "import ray\n",
        "from ray.rllib.agents.dqn import DQNTrainer, ApexTrainer, SimpleQTrainer, R2D2Trainer \n",
        "from ray.rllib.agents.ppo import PPOTrainer\n",
        "from ray.rllib.agents.a3c import A3CTrainer\n",
        "from ray.rllib.agents.pg import PGTrainer\n",
        "from ray.rllib.agents.impala import ImpalaTrainer\n",
        "import matplotlib.patheffects as pe\n",
        "from scipy.special import softmax\n",
        "import pickle\n",
        "from scipy.stats import norm, truncnorm\n",
        "import logging\n",
        "from ray import tune\n",
        "from ray.rllib.agents.trainer_template import build_trainer\n",
        "import scipy.stats as stats\n",
        "import matplotlib as mpl\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def sigmoid(i):  \n",
        "    return np.exp(-np.logaddexp(0, -i))\n",
        "\n",
        "politics_color_mapping = ['darkblue', 'royalblue', 'lightsteelblue', 'thistle', 'lightcoral', 'crimson', 'darkred']\n",
        "politics_labels = [\"far left\", \"left\", \"lean left\", \"center\", \"lean right\", \"right\", \"far right\"]\n",
        "politics_labels_capital = [\"Far Left\", \"Left\", \"Lean Left\", \"Center\", \"Lean Right\", \"Right\", \"Far Right\"]"
      ],
      "id": "624654d8-aed4-418b-9506-c2d48f36a6c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUaRL_bdKzBL"
      },
      "source": [
        "## simulation"
      ],
      "id": "QUaRL_bdKzBL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MPeKc4GtsW6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "n_categories = 7\n",
        "\n",
        "def encode_politics(politics):\n",
        "    i = np.clip(int(np.floor((politics+1)/(2/7))), 0, 6)\n",
        "    oh_politics = np.zeros(7, dtype=int)\n",
        "    oh_politics[i] = 1\n",
        "    return oh_politics\n",
        "\n",
        "def decode_politics(politics):\n",
        "    assert len(politics)==7\n",
        "    i = np.argmax(politics)\n",
        "    start = i * (2/7) - 1\n",
        "    end = start + (2/7)\n",
        "    return (i, start, end)\n",
        "\n",
        "class User:\n",
        "    def __init__(self, belief=None, malleability=None, polarization_factor=None, random_seed=None, mu_l=-0.5, sigma_l=0.25, mu_r=0.3, sigma_r=0.3, p_l=0.55):\n",
        "        self.random_seed = random_seed if random_seed is not None else np.random.randint(0, 10000)\n",
        "        self.user_id = str(uuid())\n",
        "        if belief is not None:\n",
        "            self.belief = belief\n",
        "        else:\n",
        "            self.identity = np.random.choice([\"L\", \"R\"], p=[p_l, 1-p_l])\n",
        "            self.belief = np.clip(np.random.normal(mu_l, sigma_l), -1, 1) if self.identity==\"L\" else  np.clip(np.random.normal(mu_r, sigma_r), -1, 1)\n",
        "            if abs(self.belief) >= 0.9:\n",
        "                self.belief = np.clip(np.random.normal(mu_l, sigma_l), -1, 1) if self.identity==\"L\" else  np.clip(np.random.normal(mu_r, sigma_r), -1, 1)\n",
        "        \n",
        "        self.malleability = np.random.uniform(0.01, 0.1)\n",
        "        self.polarization_factor = 1 #np.random.uniform(0.25, 1.25) #np.random.uniform(0.7, 0.9) #polarization_factor if polarization_factor else 0.8 # np.random.choice([0.25, 0.75, 1.25])polarization_factor if polarization_factor else np.random.uniform(0.25, 1.25)\n",
        "        self.engagement = 0\n",
        "        self.satisfaction = 1\n",
        "        self.open_mindedness = 0.5\n",
        "        self.partisan_click_factor = 0.5\n",
        "\n",
        "        self.is_active = True\n",
        "\n",
        "        self.initial_belief = self.belief\n",
        "        self.content_history, self.click_history, self.belief_history, self.action_history = [],[],[],[]\n",
        "        self.n_clicks = 0\n",
        "\n",
        "\n",
        "class Content:\n",
        "    def __init__(self, bias):\n",
        "        politics_mapping = {\n",
        "          \"extreme left\": {'int': 0, 'center': -0.75},\n",
        "          \"left\": {'int': 1, 'center': -0.50},\n",
        "          \"slight left\": {'int': 2, 'center': -0.25}, \n",
        "          \"neutral\": {'int': 3, 'center': 0},\n",
        "          \"slight right\": {'int': 4, 'center': 0.25},\n",
        "          \"right\": {'int': 5, 'center': 0.50},\n",
        "          \"extreme right\": {'int': 6, 'center': 0.75}\n",
        "        }\n",
        "        if type(bias)==str:\n",
        "            self.politics_name = bias\n",
        "            politics_encoding = np.zeros(7)\n",
        "            politics_encoding[politics_mapping[bias]['int']] = 1\n",
        "        else:\n",
        "            politics_encoding = bias\n",
        "        assert len(politics_encoding)==7\n",
        "        self.politics_int, self.politics_start, self.politics_end = decode_politics(politics_encoding) \n",
        "        self.politics_name = [k for k,v in politics_mapping.items() if v['int']==self.politics_int][0]\n",
        "        center = -0.75 + self.politics_int*0.25\n",
        "        center = (self.politics_start + self.politics_end)/2\n",
        "        self.politics = np.random.uniform(self.politics_start, self.politics_end)\n",
        "\n",
        "def recommend_content(user, content, x=0.5, max_click_probability=0.8, probability_spread=10):\n",
        "\n",
        "    ## opinion shift\n",
        "    dissonance = content.politics - user['belief']    \n",
        "    extremes_decay = (1 - abs(user['belief'])**2) if user['belief']*(dissonance - dissonance**3) > 0 else 1\n",
        "    shift = dissonance*(1 - (dissonance**2)/(user['polarization_factor']**2))*user['malleability']*user['engagement']*extremes_decay\n",
        "    user['belief'] += shift\n",
        "\n",
        "    ## click probability\n",
        "    opposition = user['belief']*content.politics\n",
        "    click_probs = sigmoid((user['open_mindedness'])/(abs(dissonance) + 1e-8))**probability_spread * max_click_probability\n",
        "    click_probs = np.clip(click_probs, 0, 1)\n",
        "    click = np.random.choice([0,1], p=[1-click_probs, click_probs]) \n",
        "    user['click_history'].append(click)\n",
        "\n",
        "    user['engagement'] += np.random.uniform(0.01, 0.1) if click else 0\n",
        "    user['satisfaction'] *= 1 + np.random.uniform(0.01, 0.1) if click else 1 - np.random.uniform(0.01, 0.1)\n",
        "\n",
        "    user['belief'] = np.clip(user['belief'], -1, 1)\n",
        "    user['satisfaction'] = np.clip(user['satisfaction'], 0, 1)\n",
        "    user['engagement'] = np.clip(user['engagement'], 0, 1)\n",
        "\n",
        "    user['content_history'].append(content.politics)\n",
        "\n",
        "    ## user attrition\n",
        "    attrition_threshold = 0.25\n",
        "    if user['satisfaction'] < attrition_threshold:\n",
        "        user_attrition_probs = 1 - user[\"satisfaction\"]/attrition_threshold\n",
        "        user_leaves_platform = np.random.choice([False,True], p=[1-user_attrition_probs, user_attrition_probs])\n",
        "        if user_leaves_platform:\n",
        "            user['is_active'] = False\n",
        "    \n",
        "    user['n_clicks'] += click\n",
        "    return user, click, click_probs"
      ],
      "id": "4MPeKc4GtsW6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qhToUZA2kPt"
      },
      "source": [
        "## visualizations"
      ],
      "id": "9qhToUZA2kPt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4qSNQz3glvs"
      },
      "source": [
        "### user belief distribution"
      ],
      "id": "u4qSNQz3glvs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9XKqZrfgcA4"
      },
      "outputs": [],
      "source": [
        "left_users = [user for user in users if user['identity']==\"L\"]\n",
        "right_users = [user for user in users if user['identity']==\"R\"]\n",
        "\n",
        "mpl.rcParams.update({'font.size': 22})\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['party'] = ['left' for user in left_users] + ['right' for user in right_users]\n",
        "df['politics'] = [user['politics'] for user in left_users] + [user['politics'] for user in right_users]\n",
        "plt.figure(figsize=(20, 10), dpi=80)\n",
        "\n",
        "plt.hist([user['politics'] for user in left_users], color='darkblue', alpha=0.75, bins=28)\n",
        "plt.hist([user['politics'] for user in right_users], color='darkred', alpha=0.75, bins=28)\n",
        "\n",
        "plt.xlim(-1,1)\n",
        "plt.xlabel('User Belief')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.title('User Distribution')\n",
        "plt.legend(labels=['Democrat', \"Republican\"])\n",
        "plt.tight_layout()\n",
        "plt.show(block=False)"
      ],
      "id": "i9XKqZrfgcA4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9Oeohb7gh4e"
      },
      "source": [
        "### average shifts"
      ],
      "id": "D9Oeohb7gh4e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avatuw6DggiS"
      },
      "outputs": [],
      "source": [
        "ps,starts,ends = [],[],[]\n",
        "d = 0.8\n",
        "for i in range(7):\n",
        "  p = np.zeros(7)\n",
        "  p[i] = 1\n",
        "  p_int, start, end = decode_politics(p)\n",
        "  ps.append(p)\n",
        "  starts.append(start)\n",
        "  ends.append(end)\n",
        "fig,axes = plt.subplots(7,figsize=(20,25), sharey=True)\n",
        "\n",
        "cmap = mpl.colors.ListedColormap(politics_color_mapping)\n",
        "polarization_factor = 0.8\n",
        "for p,s,e,ax in zip(ps,starts,ends,axes):\n",
        "  user_beliefs = [User(np.random.uniform(s,e)).belief for _ in range(100)]\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "  shifts,cs,categories = [],[],[]\n",
        "  for c in range(7):\n",
        "    for u in user_beliefs:\n",
        "        user = User(u).__dict__\n",
        "        user['engagement'] = 1\n",
        "        user['polarization_factor'] = polarization_factor\n",
        "        content_politics = np.zeros(7)\n",
        "        content_politics[c] = 1\n",
        "        content = Content(content_politics)\n",
        "        updated_user, _, _ = recommend_content(user, content)\n",
        "        shift = user['belief'] - updated_user['initial_belief']\n",
        "        shifts.append(shift)\n",
        "        cs.append(content.politics)\n",
        "        categories.append(c)\n",
        "  xmin,xmax = -np.max([abs(np.min(shifts)), abs(np.max(shifts))]), np.max([abs(np.min(shifts)), abs(np.max(shifts))])\n",
        "  ax.imshow([[-1,1.], [-1,1.]], aspect='auto', cmap = \"coolwarm\", interpolation = 'bicubic', extent=[xmin, xmax, -1.1, 1.1], alpha=xmax)\n",
        "  df['shift'] = shifts\n",
        "  df['content_politics'] = cs\n",
        "  df['category'] = categories\n",
        "  scatter = ax.scatter(data=df, x='shift', y='content_politics', c='category', cmap=cmap)\n",
        "\n",
        "  ax.set_title(f\"{politics_labels[np.argmax(p)]} user\", weight='bold')\n",
        "  \n",
        "  ax.set_ylabel('content politics')\n",
        "  ax.set_xlabel('shift')\n",
        "  ax.set_xlim(xmin,xmax)\n",
        "  ax.text(xmax*0.5, 0, \"shift right\")\n",
        "  ax.text(xmin*0.5, 0, \"shift left\")\n",
        "\n",
        "plt.tight_layout(h_pad=2)\n",
        "fig.subplots_adjust(right=0.8)\n",
        "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
        "cbar=fig.colorbar(scatter, cax=cbar_ax)\n",
        "cbar.set_ticks([0.5, 1.3, 2.2, 3, 3.8, 4.7, 5.5])\n",
        "cbar.set_ticklabels(politics_labels)\n",
        "cbar.set_label('content politics', labelpad=-75, y=1.05, rotation=0, weight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "id": "Avatuw6DggiS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnJKZNDIn8Fx"
      },
      "outputs": [],
      "source": [
        "ps,starts,ends = [],[],[]\n",
        "d = 0.8\n",
        "for i in range(7):\n",
        "  p = np.zeros(7)\n",
        "  p[i] = 1\n",
        "  p_int, start, end = decode_politics(p)\n",
        "  ps.append(p)\n",
        "  starts.append(start)\n",
        "  ends.append(end)\n",
        "fig,axes = plt.subplots(7, figsize=(30,35), sharey=True)\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "cmap = mpl.colors.ListedColormap(politics_color_mapping)\n",
        "polarization_factor = 1.25\n",
        "xlim = 0.1\n",
        "for p,s,e,user_ax in zip(ps,starts,ends,axes):\n",
        "  user_beliefs = [User(np.random.uniform(s,e)).belief for _ in range(1000)]\n",
        "  df = pd.DataFrame()\n",
        "  shifts,cs,categories = [],[],[]\n",
        "  for c in range(7):\n",
        "    for u in user_beliefs:\n",
        "        user = User(u).__dict__\n",
        "        user['engagement'] = 1\n",
        "        user['polarization_factor'] = 1\n",
        "        content_politics = np.zeros(7)\n",
        "        content_politics[c] = 1\n",
        "        content = Content(content_politics)\n",
        "        user, _, _ = recommend_content(user, content)\n",
        "        shift = user['belief'] - user['initial_belief']\n",
        "        shifts.append(shift)\n",
        "        cs.append(content.politics)\n",
        "        categories.append(c)\n",
        "  xmin,xmax = -np.max([abs(np.min(shifts)), abs(np.max(shifts))]), np.max([abs(np.min(shifts)), abs(np.max(shifts))])\n",
        "  user_ax.imshow([[-1,1.], [-1,1.]], aspect='auto', cmap =\"coolwarm\", interpolation ='bicubic', extent=[-xlim, xlim, -1.1, 1.1], alpha=0.25)\n",
        "  user_ax.set_title(f\"{politics_labels[np.argmax(p)]} user\", weight='bold')\n",
        "  df['shift'] = shifts\n",
        "  df['content_politics'] = cs\n",
        "  df['category'] = categories\n",
        "  scatter = user_ax.scatter(data=df, x='shift', y='content_politics', c='category', cmap=cmap)\n",
        "  user_ax.vlines(x=0, ymin=-1, ymax=1, colors='black')\n",
        "  user_ax.set_title(f\"{politics_labels[np.argmax(p)]} user\", weight='bold')\n",
        "\n",
        "  user_ax.set_xlim(-xlim,xlim)\n",
        "  user_ax.set_xticks([-xlim, 0, xlim])\n",
        "  if np.argmax(p)==3:\n",
        "      user_ax.set_ylabel('content bias', weight='bold')\n",
        "  if np.argmax(p)==6:\n",
        "      user_ax.set_xlabel('user belief shift', weight='bold')\n",
        "\n",
        "plt.tight_layout(h_pad=1)\n",
        "fig.subplots_adjust(right=0.8)\n",
        "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
        "cbar=fig.colorbar(scatter, cax=cbar_ax)\n",
        "cbar.set_ticks([0.5, 1.3, 2.2, 3, 3.8, 4.7, 5.5])\n",
        "cbar.set_ticklabels(politics_labels)\n",
        "cbar.set_label('content', labelpad=-75, y=1.05, rotation=0, weight='bold')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "id": "QnJKZNDIn8Fx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yNXKWA5YEho"
      },
      "outputs": [],
      "source": [
        "ps,starts,ends = [],[],[]\n",
        "d = 0.8\n",
        "for i in range(7):\n",
        "  p = np.zeros(7)\n",
        "  p[i] = 1\n",
        "  p_int, start, end = decode_politics(p)\n",
        "  ps.append(p)\n",
        "  starts.append(start)\n",
        "  ends.append(end)\n",
        "fig,axes = plt.subplots(7, 3, figsize=(20,25), sharey=True)\n",
        "plt.rcParams.update({'font.size': 10})\n",
        "cmap = mpl.colors.ListedColormap(politics_color_mapping)\n",
        "polarization_factor = 1.25\n",
        "for p,s,e,user_ax in zip(ps,starts,ends,axes):\n",
        "  for ax,polarization_factor,idx,xlim in zip(user_ax, [0.25, 0.5, 0.75], [0,1,2], [0.02, 0.02, 0.02]):\n",
        "      user_beliefs = [User(np.random.uniform(s,e)).belief for _ in range(100)]\n",
        "\n",
        "      df = pd.DataFrame()\n",
        "      shifts,cs,categories = [],[],[]\n",
        "      for c in range(7):\n",
        "        for u in user_beliefs:\n",
        "            user = User(u).__dict__\n",
        "            user['engagement'] = 1\n",
        "            # user['polarization_factor'] = polarization_factor\n",
        "            content_politics = np.zeros(7)\n",
        "            content_politics[c] = 1\n",
        "            content = Content(content_politics)\n",
        "            user, _, _ = recommend_content(user, content)\n",
        "            shift = user['belief'] - user['initial_belief']\n",
        "            shifts.append(shift)\n",
        "            cs.append(content.politics)\n",
        "            categories.append(c)\n",
        "      xmin,xmax = -np.max([abs(np.min(shifts)), abs(np.max(shifts))]), np.max([abs(np.min(shifts)), abs(np.max(shifts))])\n",
        "      ax.imshow([[-1,1.], [-1,1.]], aspect='auto', cmap =\"coolwarm\", interpolation ='bicubic', extent=[-xlim, xlim, -1.1, 1.1], alpha=0.25)\n",
        "      df['shift'] = shifts\n",
        "      df['content_politics'] = cs\n",
        "      df['category'] = categories\n",
        "      scatter = ax.scatter(data=df, x='shift', y='content_politics', c='category', cmap=cmap)\n",
        "      ax.vlines(x=0, ymin=-1, ymax=1, colors='black')\n",
        "      if idx==1:\n",
        "          ax.set_title(f\"{politics_labels[np.argmax(p)]} user\", weight='bold')\n",
        "\n",
        "      ax.set_xlim(-xlim,xlim)\n",
        "      ax.set_xticks([-xlim, 0, xlim])\n",
        "      if np.argmax(p)==3 and idx==0:\n",
        "          ax.set_ylabel('content', weight='bold')\n",
        "      if np.argmax(p)==6 and idx==1:\n",
        "          ax.set_xlabel('shift', weight='bold')\n",
        "\n",
        "plt.tight_layout(h_pad=1)\n",
        "fig.subplots_adjust(right=0.8)\n",
        "cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
        "cbar=fig.colorbar(scatter, cax=cbar_ax)\n",
        "cbar.set_ticks([0.5, 1.3, 2.2, 3, 3.8, 4.7, 5.5])\n",
        "cbar.set_ticklabels(politics_labels)\n",
        "cbar.set_label('content', labelpad=-75, y=1.05, rotation=0, weight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "id": "1yNXKWA5YEho"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9QhUUV4gu8c"
      },
      "outputs": [],
      "source": [
        ".# from scipy.spatial import ConvexHull\n",
        "# ps,starts,ends = [],[],[]\n",
        "# d = 1.25\n",
        "# for i in range(7):\n",
        "#   p = np.zeros(7)\n",
        "#   p[i] = 1\n",
        "#   p_int, start, end = decode_politics(p)\n",
        "#   ps.append(p)\n",
        "#   starts.append(start)\n",
        "#   ends.append(end)\n",
        "\n",
        "# cmap = mpl.colors.ListedColormap(politics_color_mapping)\n",
        "\n",
        "# fig,axes = plt.subplots(7,figsize=(20,25), sharey=True)\n",
        "# for u,ax in zip(range(7),axes):\n",
        "#   all_shifts = []\n",
        "#   for c in range(7):\n",
        "#     df = pd.DataFrame()\n",
        "#     shifts,cs,categories = [],[],[]\n",
        "#     for content_politics in [starts[c], ends[c]]:\n",
        "#       for user_politics in [starts[u],ends[u]]: \n",
        "#           for malleability in [0.001, 0.01, 0.1]:\n",
        "#                 user = User(user_politics).__dict__\n",
        "#                 user['malleability'] = malleability\n",
        "#                 user['engaged'] = 1\n",
        "#                 content = Content(\"neutral\")\n",
        "#                 content.politics = content_politics\n",
        "#                 updated_user, _, _ = recommend_content(user, content, clip=True)\n",
        "#                 shift = user['politics'] - updated_user['initial_belief']\n",
        "#                 shifts.append(shift)\n",
        "#                 cs.append(content.politics)\n",
        "#                 categories.append(c)\n",
        "#     df['shift'] = shifts\n",
        "#     df['content_politics'] = cs\n",
        "#     df['category'] = categories\n",
        "#     # ax.scatter(data=df, x='shift', y='content_politics', c='category', cmap=cmap)\n",
        "\n",
        "#     points = np.array([[shift,p] for shift,p in zip(shifts, cs)])\n",
        "\n",
        "#     hull = ConvexHull(points)\n",
        "\n",
        "#     ax.fill(points[hull.vertices,0], points[hull.vertices,1],politics_color_mapping[c])\n",
        "#     all_shifts = all_shifts + shifts\n",
        "#   # ax.fill_between(df['shift'], df['content_politics'])\n",
        "#   xmin,xmax = -np.max([abs(np.min(all_shifts)), abs(np.max(all_shifts))]), np.max([abs(np.min(all_shifts)), abs(np.max(all_shifts))])\n",
        "#   ax.imshow([[-1,1.], [-1,1.]], aspect='auto', cmap = \"coolwarm\", interpolation = 'bicubic', extent=[xmin, xmax, -1.1, 1.1], alpha=xmax)\n",
        "#   ax.set_xlim(xmin-0.001,xmax+0.001)\n",
        "# plt.show()"
      ],
      "id": "l9QhUUV4gu8c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt-9QwBVgyBZ"
      },
      "source": [
        "### click probability distributions"
      ],
      "id": "qt-9QwBVgyBZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpfYHlSRw9mz"
      },
      "outputs": [],
      "source": [
        "starts = [i * (2/7) - 1 for i in range(7)]\n",
        "ends = [start + (2/7) for start in starts]\n",
        "mids = [np.mean([start,end]) for start,end in zip(starts,ends)]\n",
        "\n",
        "fig,axes = plt.subplots(4,2, figsize=(30,20))\n",
        "mpl.rcParams.update({'font.size': 12})\n",
        "\n",
        "for user_belief,label,ax in zip([-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75], politics_labels_capital, [axes[0][0], axes[1][0], axes[2][0], axes[3][0], axes[0][1], axes[1][1], axes[2][1]]):\n",
        "    beliefs, probs = [], []\n",
        "    for start,end,color in zip(starts, ends ,politics_color_mapping):\n",
        "        ax.fill_between([start, end], [1, 1], color=color, alpha=0.75)\n",
        "\n",
        "    for i in range(7):\n",
        "        for _ in range(1000):\n",
        "            user = User(belief=user_belief)\n",
        "\n",
        "            content_politics = np.zeros(7)\n",
        "            content_politics[i] = 1\n",
        "            content = Content(content_politics)\n",
        "\n",
        "            user, click, click_probs = recommend_content(user.__dict__, content)\n",
        "            beliefs.append(content.politics)\n",
        "            probs.append(click_probs)\n",
        "    \n",
        "    ax.scatter(beliefs, probs, color='black')\n",
        "\n",
        "    content_boundaries = [i * (2/7) - 1 for i in range(7)]\n",
        "\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_xlim(-1, 1)\n",
        "\n",
        "    ax.set_title(f'{label} User (b={np.round(user_belief,2)})')\n",
        "    ax.set_xlabel('Content Bias')\n",
        "    ax.set_ylabel('Click Probability')\n",
        "fig.suptitle('Click Probability Distributions', y=1.005, x=0.51, horizontalalignment='center', weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fpfYHlSRw9mz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CljFyomZgEDQ"
      },
      "outputs": [],
      "source": [
        "color_mapping = ['darkblue', 'royalblue', 'lightsteelblue', 'thistle', 'lightcoral', 'crimson', 'darkred']\n",
        "starts = [i * (2/7) - 1 for i in range(7)]\n",
        "ends = [start + (2/7) for start in starts]\n",
        "mids = [np.mean([start,end]) for start,end in zip(starts,ends)]\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.xticks(starts + [1])\n",
        "plt.bar(mids, [1 for _ in range(7)], color=color_mapping,  width=2/7)\n",
        "for i in range(7):\n",
        "  plt.text(x=mids[i], y=0.5, s=politics_labels_capital[i], horizontalalignment='center', color='white', fontdict={'fontsize':30}) # path_effects=[pe.withStroke(linewidth=4, foreground=\"black\")])\n",
        "plt.xlabel('Bias')\n",
        "plt.yticks([])\n",
        "\n",
        "plt.xlim(-1,1)\n",
        "plt.ylim(0,1)\n",
        "plt.tight_layout()\n",
        "plt.title('Content Politics')\n",
        "plt.show()"
      ],
      "id": "CljFyomZgEDQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01w8VUMbK1de"
      },
      "source": [
        "## gym environment"
      ],
      "id": "01w8VUMbK1de"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARQQLVR9jK6o"
      },
      "outputs": [],
      "source": [
        "def get_env():\n",
        "    class NewsRecommendationEnv(gym.Env):\n",
        "        def __init__(self, env_config):        \n",
        "            # Defines the length of the episode in terms of no. recommendations (we use horizon=30 in our experiments)\n",
        "            self.horizon = env_config['horizon'] if 'horizon' in env_config else 100\n",
        "            self.target_politics = env_config['target_politics'] if 'target_politics' in env_config else None\n",
        "            self.mode = env_config['mode'] if 'mode' in env_config else None\n",
        "            self.eval = env_config['eval'] if 'eval' in env_config else None\n",
        "            self.target_politics_reward = 1 if 'target_politics_reward' not in env_config else env_config['target_politics_reward']\n",
        "            self.other_reward = 0 if 'other_reward' not in env_config else env_config['other_reward']\n",
        "            self.session_length = 500 if 'session_length' not in env_config else env_config['session_length']\n",
        "\n",
        "            self.action_space = gym.spaces.Discrete(7)\n",
        "            self.observation_space = gym.spaces.Box(0, 1, shape=(self.horizon,8), dtype=np.int64) #gym.spaces.Box(0, self.session_length, shape=(14,), dtype=np.int64)\n",
        "\n",
        "            self.current_user = None\n",
        "\n",
        "            self.session_history_tracker = ray.get_actor(env_config['actor_name']) if 'actor_name' in env_config else None\n",
        "\n",
        "        def update_user_state(self, state, action, click):\n",
        "            record = np.zeros(8)\n",
        "            record[action] = 1 \n",
        "            record[7] = 1 if click else 0\n",
        "            state = np.concatenate([[record],state[:-1]])\n",
        "            return state\n",
        "      \n",
        "        def record_session(self):\n",
        "            ray.get(self.session_history_tracker.record_session.remote(time.time(), self.current_session, copy.deepcopy(self.current_user)))\n",
        "\n",
        "        def reset(self, user=None):\n",
        "            if user:\n",
        "                self.current_user = user\n",
        "            else:\n",
        "                self.current_user = User().__dict__\n",
        "                \n",
        "            if 'state' not in self.current_user:\n",
        "                self.current_user['state'] = np.zeros(8*self.horizon).reshape(self.horizon, 8)\n",
        "            self.current_user['engaged'] = False\n",
        "            self.current_user['is_active'] = True\n",
        "            self.current_session = {\n",
        "              'initial_belief': self.current_user['belief'], \n",
        "              'end_belief': self.current_user['belief'],\n",
        "              'clicks': 0,\n",
        "              'n': 0,\n",
        "              'ctr': 0,\n",
        "              'rewards': 0,\n",
        "              'user_id': self.current_user['user_id']\n",
        "            }\n",
        "\n",
        "            return self.current_user['state']\n",
        "\n",
        "        def step(self, action):\n",
        "          \n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "            content_politics = np.zeros(7)\n",
        "            content_politics[action] = 1\n",
        "            \n",
        "            content = Content(content_politics)\n",
        "            self.current_user, click, click_probs = recommend_content(self.current_user, content)\n",
        "            if click:\n",
        "              if self.mode == \"manipulate\":\n",
        "                  reward += self.target_politics_reward if content.politics_int in self.target_politics else self.other_reward\n",
        "              else:\n",
        "                  reward += 1\n",
        "            \n",
        "            self.current_user['state'] = self.update_user_state(self.current_user['state'], action, click)\n",
        "\n",
        "            self.current_session['n'] += 1\n",
        "            self.current_session['clicks'] += click\n",
        "            self.current_session['rewards'] += reward\n",
        "            self.current_session['clicked'] = click\n",
        "\n",
        "            if self.current_session['n']>=self.session_length:\n",
        "                done = True\n",
        "            if not self.current_user['is_active']:\n",
        "                done = True\n",
        "                reward = -1\n",
        "\n",
        "            if done:\n",
        "                self.current_session['end_belief'] = self.current_user['belief']\n",
        "                self.current_session['belief_shift'] = self.current_session['end_belief'] - self.current_session['initial_belief']\n",
        "                self.current_session['ctr'] = self.current_session['clicks'] / self.current_session['n']\n",
        "                if self.session_history_tracker:\n",
        "                    self.record_session()\n",
        "\n",
        "            return self.current_user['state'], reward, done, self.current_session\n",
        "\n",
        "    return NewsRecommendationEnv"
      ],
      "id": "ARQQLVR9jK6o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xbd4SUhqvhU1"
      },
      "outputs": [],
      "source": [
        "NewsRecommendationEnv = get_env()\n",
        "env = NewsRecommendationEnv({'horizon':10, 'session_length':500})\n",
        "env.reset()"
      ],
      "id": "Xbd4SUhqvhU1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2GooRYrb5As"
      },
      "source": [
        "### here"
      ],
      "id": "q2GooRYrb5As"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15qAHiHIjILh"
      },
      "outputs": [],
      "source": [
        "shifts = []\n",
        "lengths = []\n",
        "sat = []\n",
        "for _ in range(100):\n",
        "    user = User().__dict__\n",
        "    while True:\n",
        "        action = np.random.choice([0,1,2,3,4,5,6])\n",
        "        content = np.zeros(7)\n",
        "        content[action] = 1\n",
        "        content = Content(content)\n",
        "        user,click,probs = recommend_content(user, content)\n",
        "        if not user['is_active'] or len(user['content_history'])>=500:\n",
        "            sat.append(user['satisfaction'])\n",
        "            break\n",
        "\n",
        "    shifts.append(user['politics'] - user['initial_belief'])\n",
        "    lengths.append(len(user['content_history']))\n",
        "sns.displot(shifts)\n",
        "sns.displot(lengths)\n",
        "sns.displot(sat)"
      ],
      "id": "15qAHiHIjILh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFkrT1ziVKv0"
      },
      "outputs": [],
      "source": [
        "p = []\n",
        "attrition = []\n",
        "lengths = []\n",
        "for _ in range(100):\n",
        "  user = User(-0.5).__dict__\n",
        "  sat = []\n",
        "  n = 0\n",
        "  while user['politics'] < 0.8:\n",
        "      sat.append(user['satisfaction'])\n",
        "      if user['politics'] < -0.25:\n",
        "          content = Content(\"extreme left\")\n",
        "      elif user['politics'] < 0:\n",
        "          content = Content(\"neutral\")\n",
        "      elif user['politics'] < 0.4:\n",
        "          content = Content(\"slight right\")\n",
        "      elif user['politics'] < 0.6:\n",
        "          content = Content(\"right\")\n",
        "      else:\n",
        "          content = Content(\"extreme right\")\n",
        "      user,click,click_probs = recommend_content(user, content)\n",
        "      n += 1\n",
        "      if not user['is_active']:\n",
        "          attrition.append(1)\n",
        "          # print(user['satisfaction'])\n",
        "          break\n",
        "      if n >= 500:\n",
        "          break\n",
        "  p.append(user['politics'])\n",
        "  lengths.append(len(user['content_history']))\n",
        "  # plt.plot([i for i in range(n)], sat)\n",
        "  # plt.show()\n",
        "sns.displot(p)\n",
        "sns.displot(lengths)\n",
        "sum(attrition)"
      ],
      "id": "SFkrT1ziVKv0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQoaFtC60_lf"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "qQoaFtC60_lf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwGtxytLSZ2n"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "LwGtxytLSZ2n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucf7pGpAtJ0U"
      },
      "outputs": [],
      "source": [
        "env.reset(User().__dict__)"
      ],
      "id": "ucf7pGpAtJ0U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqettcRbtYBg"
      },
      "outputs": [],
      "source": [
        "env.current_user['politics']"
      ],
      "id": "sqettcRbtYBg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtkGAmKvtdql"
      },
      "outputs": [],
      "source": [
        "env.step(2)[0].shape"
      ],
      "id": "qtkGAmKvtdql"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce_T5cYgtOU-"
      },
      "outputs": [],
      "source": [
        "env.step(1)"
      ],
      "id": "Ce_T5cYgtOU-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfvbBLaMOz06",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "## avg shift acting randomly\n",
        "shifts = []\n",
        "start = time.time()\n",
        "for _ in range(1000):\n",
        "    state = env.reset(User().__dict__)\n",
        "    done = False\n",
        "    while not done:\n",
        "        state, reward, done, info = env.step(np.random.choice([i for i in range(7)]))\n",
        "    shifts.append(env.current_user['politics'] - env.current_user['initial_belief'])\n",
        "sns.displot(shifts, kde=True)"
      ],
      "id": "xfvbBLaMOz06"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKFNDCmbK3Et"
      },
      "source": [
        "## random agent"
      ],
      "id": "AKFNDCmbK3Et"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NXUmCbMRvWb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from ray.rllib.agents.trainer import Trainer, with_common_config\n",
        "from ray.rllib.utils.annotations import override\n",
        "from ray.rllib.utils.typing import TrainerConfigDict\n",
        "\n",
        "\n",
        "class RandomAgent(Trainer):\n",
        "    \"\"\"Trainer that produces random actions and never learns.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    @override(Trainer)\n",
        "    def get_default_config(cls) -> TrainerConfigDict:\n",
        "        return with_common_config({\n",
        "            \"rollouts_per_iteration\": 10,\n",
        "            \"framework\": \"tf\",  # not used\n",
        "        })\n",
        "\n",
        "    @override(Trainer)\n",
        "    def _init(self, config, env_creator):\n",
        "        self.env = env_creator(config[\"env_config\"])\n",
        "\n",
        "    class RandomPolicy():\n",
        "        def __init__(self, env):\n",
        "            self.env = env\n",
        "        def compute_actions(self, obs_batch):\n",
        "            return [[self.env.action_space.sample() for _ in obs_batch]]\n",
        "    \n",
        "    @override(Trainer)\n",
        "    def get_policy(self):\n",
        "        return self.RandomPolicy(self.env)\n",
        "\n",
        "    @override(Trainer)\n",
        "    def step(self):\n",
        "        rewards = []\n",
        "        steps = 0\n",
        "        for _ in range(self.config[\"rollouts_per_iteration\"]):\n",
        "            obs = self.env.reset()\n",
        "            done = False\n",
        "            reward = 0.0\n",
        "            while not done:\n",
        "                action = self.env.action_space.sample()\n",
        "                obs, r, done, info = self.env.step(action)\n",
        "                reward += r\n",
        "                steps += 1\n",
        "            rewards.append(reward)\n",
        "        return {\n",
        "            \"episode_reward_mean\": np.mean(rewards),\n",
        "            \"timesteps_this_iter\": steps,\n",
        "        }\n",
        "    @override(Trainer)\n",
        "    def compute_single_action(self, state, full_fetch=False):\n",
        "        if full_fetch:\n",
        "            action = self.env.action_space.sample()\n",
        "            q_values = np.zeros(self.env.action_space.n)\n",
        "            q_values[action] = 1\n",
        "            d = [{'action': action}, {'None': 'None'}, {'q_values': q_values}]\n",
        "            return d\n",
        "        return self.env.action_space.sample()"
      ],
      "id": "5NXUmCbMRvWb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUxx5ANFtMmm"
      },
      "outputs": [],
      "source": [
        "def compute_single_action(state):\n",
        "    clicks = [int(v) for i,v in enumerate(state) if i%2==1 and v!=-1]\n",
        "    contents = [int(v) for i,v in enumerate(state) if i%2==0 and v!=-1]\n",
        "    content_clicks = {i: 0 for i in range(7)}\n",
        "    for content,click in zip(contents,clicks):\n",
        "        print(content,click)\n",
        "        content_clicks[content] += click\n",
        "\n",
        "    action = next(iter([content for content in range(7) if contents.count(content)<5 or content_clicks[content]>2]))\n",
        "    return action"
      ],
      "id": "SUxx5ANFtMmm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_aVpBKRZ03P"
      },
      "source": [
        "## baseline agent"
      ],
      "id": "E_aVpBKRZ03P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXlyIk7jBXVA"
      },
      "outputs": [],
      "source": [
        "class BaselineAgent(Trainer):\n",
        "    \"\"\"Trainer that produces random actions and never learns.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    @override(Trainer)\n",
        "    def get_default_config(cls) -> TrainerConfigDict:\n",
        "        return with_common_config({\n",
        "            \"rollouts_per_iteration\": 10,\n",
        "            \"framework\": \"tf\",  # not used\n",
        "        })\n",
        "\n",
        "    @override(Trainer)\n",
        "    def _init(self, config, env_creator):\n",
        "        self.env = env_creator(config[\"env_config\"])\n",
        "\n",
        "    class BaselinePolicy():\n",
        "        def __init__(self, env):\n",
        "            self.env = env\n",
        "        def compute_actions(self, obs_batch):\n",
        "            return [[self.compute_single_action(_) for _ in obs_batch]]\n",
        "            \n",
        "        def compute_single_action(self, state):\n",
        "            ctrs = [[] for _ in range(7)]\n",
        "            for s in state:\n",
        "                ctrs[np.argmax(s[:7])].append(s[7])\n",
        "            ctrs = [np.mean(ctr)**2 if len(ctr) > 0 else 0 for ctr in ctrs]\n",
        "            recs_count = len([s for s in state if sum(s[:7]) > 0])\n",
        "            if recs_count < 25:\n",
        "                probs = softmax(ctrs)\n",
        "                action = np.random.choice([0,1,2,3,4,5,6], p=probs)\n",
        "            else:\n",
        "                action = np.argmax(ctrs)\n",
        "            return action\n",
        "\n",
        "    @override(Trainer)\n",
        "    def get_policy(self):\n",
        "        return self.BaselinePolicy(self.env)\n",
        "\n",
        "    @override(Trainer)\n",
        "    def step(self):\n",
        "        rewards = []\n",
        "        steps = 0\n",
        "        for _ in range(self.config[\"rollouts_per_iteration\"]):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            reward = 0.0\n",
        "            while not done:\n",
        "                action = self.compute_single_action(state)\n",
        "                state, r, done, info = self.env.step(action)\n",
        "                reward += r\n",
        "                steps += 1\n",
        "            rewards.append(reward)\n",
        "        return {\n",
        "            \"episode_reward_mean\": np.mean(rewards),\n",
        "            \"timesteps_this_iter\": steps,\n",
        "        }\n",
        "    @override(Trainer)\n",
        "    def compute_single_action(self, state, full_fetch=False):\n",
        "        ctrs = [[] for _ in range(7)]\n",
        "        for s in state:\n",
        "            ctrs[np.argmax(s[:7])].append(s[7])\n",
        "        ctrs = [np.mean(ctr)**2 if len(ctr) > 0 else 0 for ctr in ctrs]\n",
        "        recs_count = len([s for s in state if sum(s[:7]) > 0])\n",
        "        if recs_count < 25:\n",
        "            probs = softmax(ctrs)\n",
        "            action = np.random.choice([0,1,2,3,4,5,6], p=probs)\n",
        "        else:\n",
        "            action = np.argmax(ctrs)\n",
        "        return action"
      ],
      "id": "JXlyIk7jBXVA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWlykVkmRyvR"
      },
      "source": [
        "## session history tracker"
      ],
      "id": "PWlykVkmRyvR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh5l8YCqlqIt"
      },
      "outputs": [],
      "source": [
        "@ray.remote\n",
        "class SessionHistoryTracker:\n",
        "    def __init__(self):\n",
        "        self.session_histories = []    \n",
        "\n",
        "    def record_session(self, timestamp, session_history, user):\n",
        "        session_history[\"time\"] = timestamp\n",
        "        self.session_histories.append(session_history)\n",
        "\n",
        "    def get_session_histories(self):\n",
        "        return self.session_histories"
      ],
      "id": "Uh5l8YCqlqIt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onr0M_QZLjwM"
      },
      "source": [
        "## evaluation"
      ],
      "id": "onr0M_QZLjwM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM8PAGksLnI2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def plot_results(session_histories, window):\n",
        "\n",
        "    initial_politics = [i['initial_belief'] for i in session_histories]\n",
        "    end_politics = [i['end_belief'] for i in session_histories]\n",
        "    belief_shift = [abs(end - start) for end,start in zip(end_politics, initial_politics)]\n",
        "    rewards = [i['rewards'] for i in session_histories]\n",
        "    ctr = [i['ctr'] for i in session_histories]\n",
        "    session_lengths = [i['n'] for i in session_histories]\n",
        "    mpl.rcParams.update({'font.size': 16})\n",
        "\n",
        "    fig, axes = plt.subplots(2,2)\n",
        "    fig.set_size_inches(35, 15)\n",
        "    for data, ax, ylabel in zip({'Average Rewards': rewards, 'Average CTR': ctr, 'Average Absolute Belief Shift': belief_shift,'Average Episode Length': session_lengths}.items(), \n",
        "                      [axes[0][0], axes[0][1], axes[1][0], axes[1][1]],\n",
        "                      ['Reward', 'CTR', 'Shift', 'Steps']):\n",
        "        feature_name, feature_data = data\n",
        "        rolling_avg = []\n",
        "        x = []\n",
        "        for i in range(window,len(feature_data)):\n",
        "            start = i-window if i>window else 0\n",
        "            rolling_avg.append(np.mean(feature_data[start:i]))\n",
        "            x.append(i)\n",
        "        ax.plot(x, rolling_avg)\n",
        "        ax.set_title(feature_name)\n",
        "        ax.set_ylabel(ylabel)\n",
        "        ax.set_xlabel('Iteration')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "PM8PAGksLnI2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_TmkwC4cgje",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def eval_trainer(trainer, users=False, n_users=100, mode=None, horizon=100, target_politics=None, ctr_window=1, session_length=500, random_cumulative_clicks=None, baseline_cumulative_clicks=None):\n",
        "\n",
        "    NewsRecommendationEnv = get_env()\n",
        "    users = copy.deepcopy(users) if users else [User().__dict__ for _ in range(n_users)]\n",
        "    envs = [NewsRecommendationEnv({'horizon':horizon, 'target_politics':target_politics, 'eval':True, 'mode': mode, 'session_length': session_length}) for _ in range(len(users))]\n",
        "    for user,env in zip(users,envs):\n",
        "        user['env'] = env\n",
        "\n",
        "    all_users = {user['user_id']: user for user in users}\n",
        "    active_users = list(all_users.keys())\n",
        "\n",
        "    action_histories = {user['user_id']: [] for user in users}\n",
        "    belief_histories = {user['user_id']: [] for user in users}\n",
        "    click_histories = {user['user_id']: [] for user in users}\n",
        "    attrition_histories = {user['user_id']: [] for user in users}\n",
        "    session_lengths = []\n",
        "\n",
        "    states = np.array([user['env'].reset(user) for user_id,user in all_users.items()])\n",
        "\n",
        "    while len(active_users) > 0:\n",
        "        actions = trainer.get_policy().compute_actions(obs_batch=states)[0]\n",
        "        states = []\n",
        "        updated_active_users = []\n",
        "        for user_id, action in zip(active_users, actions):\n",
        "            user = all_users[user_id]\n",
        "            env = user['env']\n",
        "            state, reward, done, info = env.step(action)\n",
        "            belief_histories[env.current_user['user_id']].append(env.current_user['belief'])\n",
        "            action_histories[env.current_user['user_id']].append(action)\n",
        "            click_histories[env.current_user['user_id']].append(env.current_session['clicked'])\n",
        "            all_users[env.current_user['user_id']] = env.current_user\n",
        "            if done:\n",
        "                session_lengths.append(env.current_session['n'])\n",
        "                if not user['is_active']:\n",
        "                    attrition_histories[env.current_user['user_id']].append(0)\n",
        "            else:\n",
        "                updated_active_users.append(env.current_user['user_id'])\n",
        "                attrition_histories[env.current_user['user_id']].append(1)\n",
        "                states.append(state)\n",
        "                user['env'] = env\n",
        "            \n",
        "        active_users = updated_active_users\n",
        "        states = np.array(states)\n",
        "\n",
        "    color_mapping = ['darkblue', 'royalblue', 'lightsteelblue', 'thistle', 'lightcoral', 'crimson', 'darkred', 'white']\n",
        "    plt.xticks([i for i in range(7)], politics_labels)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.bar([i for i in range(7)], [1 for _ in range(7)], color=color_mapping, width=1.0)\n",
        "    plt.show()\n",
        "    overall_shifts = []\n",
        "    fig,axes = plt.subplots(7,3,figsize=(40,40))\n",
        "    for p,ax in zip(range(7),axes):\n",
        "        most_common_actions = []\n",
        "        average_beliefs = []\n",
        "        actions,beliefs,clicks,attritions = [],[],[],[]\n",
        "        for belief_history,action_history,click_history,attrition_history in zip(belief_histories.values(), action_histories.values(), click_histories.values(), attrition_histories.values()):\n",
        "            if np.argmax(encode_politics(belief_history[0]))==p:\n",
        "                beliefs.append(belief_history)\n",
        "                actions.append(action_history)\n",
        "                clicks.append(click_history)\n",
        "                attritions.append(attrition_history)\n",
        "        for step in range(500):\n",
        "            actions_at_step = [action_history[step] for action_history in actions if len(action_history)>step]\n",
        "            belief_at_step = [belief_history[step] for belief_history in beliefs if len(belief_history)>step]\n",
        "            if len(actions_at_step)==0:\n",
        "                break\n",
        "            most_common_action = stats.mode(actions_at_step)\n",
        "            avg_belief = np.mean(belief_at_step)\n",
        "            if len(most_common_action.mode) == 0:\n",
        "                most_common_actions.append(7)\n",
        "            else:\n",
        "                most_common_actions.append(most_common_action.mode[0])\n",
        "            average_beliefs.append(avg_belief)\n",
        "\n",
        "        rolling_avg_ctrs = []\n",
        "        for step in range(len(most_common_actions)):\n",
        "            ctrs = []\n",
        "            for click_history in clicks:\n",
        "                ctr = np.mean(click_history[:step])\n",
        "                ctrs.append(ctr)\n",
        "            rolling_avg_ctrs.append(np.mean(ctrs))\n",
        "\n",
        "        while len(most_common_actions)<500:\n",
        "            most_common_actions.append(-1)\n",
        "\n",
        "        colors = [\"lightgrey\" if action==-1 else color_mapping[action] for action in most_common_actions]\n",
        "\n",
        "        mpl.rcParams.update({'font.size': 30})\n",
        "\n",
        "        ax[0].bar([step + 1 for step in range(50)], \n",
        "                  [1.05 for _ in most_common_actions[:50]], \n",
        "                  color=colors, width=1.0)            \n",
        "        # ax[0].plot([step for step in range(len(rolling_avg_ctrs[:50]))], \n",
        "        #             rolling_avg_ctrs[:50], color='green', linewidth=3, \n",
        "        #             path_effects=[pe.Stroke(linewidth=5, foreground='white'), pe.Normal()],\n",
        "        #           )\n",
        "        # ax[0].set_ylabel(\"CTR\")\n",
        "        ax[0].set_xlabel(\"Step\")\n",
        "        ax[0].set_yticks([])\n",
        "        ax[0].set_ylim(0, 1.05)\n",
        "        ax[0].set_ylabel(f'{politics_labels_capital[p]} Users',  rotation=0, fontsize=40, labelpad=175, horizontalalignment='right')\n",
        "        ax[0].set_xticks([1, 10, 20, 30, 40, 50])\n",
        "\n",
        "\n",
        "        ax[1].bar([step + 1for step in range(len(most_common_actions))], [1.05 for _ in most_common_actions], color=colors, width=1.0)\n",
        "            \n",
        "        ax[1].plot([step + 1for step in range(len(rolling_avg_ctrs))], \n",
        "                      rolling_avg_ctrs, color='green', linewidth=3, path_effects=[pe.Stroke(linewidth=5, foreground='white'), pe.Normal()],\n",
        "                      )\n",
        "        ax[1].set_ylabel(\"CTR\")\n",
        "        ax[1].set_xlabel(\"Step\")\n",
        "        ax[1].set_ylim(0, 1.05)\n",
        "        ax[1].set_xticks([1, 100, 200, 300, 400, 500])\n",
        "\n",
        "\n",
        "        ## belief shift by politics\n",
        "        for user_b in beliefs:\n",
        "            ax[2].plot([step for step in range(len(user_b))], \n",
        "                       user_b, color=politics_color_mapping[p])\n",
        "\n",
        "        ax[2].set_ylim(-1,1)\n",
        "        ax[2].set_ylabel(\"Belief\")\n",
        "        # ax[2].plot([0, len(rolling_avg_ctrs)], \n",
        "        #            [np.mean([user_b[0] for user_b in beliefs]), np.mean([user_b[-1] for user_b in beliefs])], \n",
        "        #            color='black', linewidth=4, path_effects=[pe.Stroke(linewidth=5, foreground='white'), pe.Normal()])\n",
        "        \n",
        "        ax2 = ax[2].twinx()\n",
        "        max_len = np.max([len(i) for i in attritions])\n",
        "        user_counts = []\n",
        "        for _ in range(max_len):\n",
        "            user_counts.append(np.sum([attrition[_] for attrition in attritions if len(attrition) > _]))\n",
        "        ax2.plot([_ for _ in range(max_len)], user_counts, color='black')\n",
        "        ax2.set_ylabel('Active Users')\n",
        "        ax2.set_ylim(0, len(attritions))\n",
        "        ax[2].set_xlabel('Step')\n",
        "\n",
        "        if p==0:\n",
        "            ax[0].set_title(\"Average Recommendations (First 50)\", pad=100)\n",
        "            ax[1].set_title(\"Average Recommendations and CTR\", pad=100)\n",
        "            ax[2].set_title(\"Belief Shift and Attrition\", pad=100)\n",
        "\n",
        "        avg_shift = np.mean([b[-1]-b[0] for b in beliefs])\n",
        "        overall_shifts.append(avg_shift)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    all_shifts = [p[-1]-p[0] for p in belief_histories.values()]\n",
        "    \n",
        "    starts = [i[0] for i in belief_histories.values()]\n",
        "    ends = [i[-1] for i in belief_histories.values()]\n",
        "\n",
        "    ## start vs end belief distribution\n",
        "    df = pd.DataFrame()\n",
        "    df['Belief'] = starts + ends\n",
        "    df['Time'] = ['Start' for _ in range(len(starts))] + ['End' for _ in range(len(ends))]\n",
        "    sns.displot(data=df, x='Belief',  hue='Time', kind=\"kde\", rug=True, height=8, aspect=15/8)\n",
        "\n",
        "    plt.title('Belief Distribution Change')\n",
        "    plt.xlim(-1, 1)\n",
        "    plt.show()\n",
        "\n",
        "    ## start vs end belief scatter\n",
        "    starts, ends = (list(t) for t in zip(*sorted(zip(starts, ends))))\n",
        "    plt.figure(figsize=(25, 5), dpi=80)\n",
        "    plt.scatter(starts, starts)\n",
        "    plt.scatter(starts, ends)\n",
        "    plt.show()\n",
        "\n",
        "    ## belief changes\n",
        "    color_mapping = ['darkblue', 'royalblue', 'lightsteelblue', 'thistle', 'lightcoral', 'crimson', 'darkred']\n",
        "    plt.figure(figsize=(25, 10), dpi=80)\n",
        "    for user in belief_histories.values():\n",
        "        plt.plot([step for step in range(len(user))], user, color=color_mapping[np.argmax(encode_politics(user[0]))])\n",
        "    plt.ylim(-1.05,1.05)\n",
        "    plt.title('Overall Belief Shifts')\n",
        "    plt.ylabel('Belief')\n",
        "    plt.xlabel('Step')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    ## average shifts\n",
        "    plt.figure(figsize=(20, 10), dpi=80)\n",
        "    shifts = [np.mean([end - start for start,end in zip(starts,ends) if np.argmax(encode_politics(start))==p]) for p in range(7)]\n",
        "    max_shift = np.max([abs(np.min(shifts)), abs(np.max(shifts))])\n",
        "    norm = plt.Normalize(-max_shift, max_shift)\n",
        "    cmap = plt.get_cmap(\"coolwarm\")\n",
        "    colors = cmap(norm(shifts))\n",
        "    plt.barh(politics_labels, shifts, color=colors)\n",
        "    plt.xlabel('avg shift')\n",
        "    plt.xlim(-max_shift - 0.05, max_shift + 0.05)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    session_lengths = [len(h) for h in belief_histories.values()]\n",
        "    print(f'avg session length: {np.mean(session_lengths)}')"
      ],
      "id": "b_TmkwC4cgje"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2URSwIMcoq1p"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_clicks(trainer, users):\n",
        "    NewsRecommendationEnv = get_env()\n",
        "    users = copy.deepcopy(users) if users else [User().__dict__ for _ in range(n_users)]\n",
        "    envs = [NewsRecommendationEnv({'eval':True}) for _ in range(len(users))]\n",
        "    for user,env in zip(users,envs):\n",
        "        user['env'] = env\n",
        "\n",
        "    all_users = {user['user_id']: user for user in users}\n",
        "    active_users = list(all_users.keys())\n",
        "\n",
        "    action_histories = {user['user_id']: [] for user in users}\n",
        "    belief_histories = {user['user_id']: [] for user in users}\n",
        "    click_histories = {user['user_id']: [] for user in users}\n",
        "    session_lengths = []\n",
        "\n",
        "    states = np.array([user['env'].reset(user) for user_id,user in all_users.items()])\n",
        "\n",
        "    while len(active_users) > 0:\n",
        "        actions = trainer.get_policy().compute_actions(obs_batch=states)[0]\n",
        "        states = []\n",
        "        updated_active_users = []\n",
        "        for user_id, action in zip(active_users, actions):\n",
        "            user = all_users[user_id]\n",
        "            env = user['env']\n",
        "            state, reward, done, info = env.step(action)\n",
        "            belief_histories[env.current_user['user_id']].append(env.current_user['belief'])\n",
        "            action_histories[env.current_user['user_id']].append(action)\n",
        "            click_histories[env.current_user['user_id']].append(env.current_session['clicked'])\n",
        "            all_users[env.current_user['user_id']] = env.current_user\n",
        "            if done:\n",
        "                session_lengths.append(env.current_session['n'])\n",
        "            else:\n",
        "                updated_active_users.append(env.current_user['user_id'])\n",
        "                states.append(state)\n",
        "                user['env'] = env\n",
        "            \n",
        "        active_users = updated_active_users\n",
        "        states = np.array(states)\n",
        "    cumulative_clicks = []\n",
        "    clicks = 0\n",
        "    for step in range(500):\n",
        "        step_clicks = np.mean([click_history[step] for click_history in click_histories.values() if len(click_history)>step])\n",
        "        clicks += step_clicks if not np.isnan(step_clicks) else 0 \n",
        "        cumulative_clicks.append(clicks)\n",
        "    return cumulative_clicks"
      ],
      "id": "2URSwIMcoq1p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dGZFLthpOGj"
      },
      "outputs": [],
      "source": [
        "cumulative_clicks = get_cumulative_clicks(random_trainer, eval_users)\n",
        "plt.plot([i for i in range(len(cumulative_clicks))], cumulative_clicks)"
      ],
      "id": "9dGZFLthpOGj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdIpqVEUoRiR"
      },
      "source": [
        "## saving"
      ],
      "id": "TdIpqVEUoRiR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhMXZKXioUT7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def save_results(session_histories, users_histories, file_name):\n",
        "    df = pd.DataFrame(session_histories)\n",
        "    df.to_csv(f'{file_name} - session_history.csv', index=False)"
      ],
      "id": "XhMXZKXioUT7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeD8F2vsK5Jy"
      },
      "source": [
        "## training function"
      ],
      "id": "KeD8F2vsK5Jy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF6koS61vhAt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def train(n_episodes, session_history_tracker, horizon, agent_name, eval_interval, log_interval, trainer, eval_users, n=None, target_politics=None, mode=None, metrics_window=1000, save_policy=True, session_length=100, checkpoint_interval=100):\n",
        "  \n",
        "    train_start = time.time()\n",
        "\n",
        "    episode_start = time.time()\n",
        "    results = []\n",
        "\n",
        "    for i in range(n_episodes):\n",
        "        result = trainer.train()\n",
        "        results.append(result)\n",
        "\n",
        "        if i%checkpoint_interval==0 and i>0:\n",
        "            t = int(time.time())\n",
        "            trainer.save(f'drive/MyDrive/thesis-training/{agent_name}-{t}/{i}')\n",
        "            session_histories = ray.get(session_history_tracker.get_session_histories.remote())\n",
        "            with open(f'drive/MyDrive/thesis-training/{agent_name}-{t}/{i}.pickle', 'wb') as handle:\n",
        "                 pickle.dump(session_histories, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "            \n",
        "        if i%log_interval==0 and i>0:\n",
        "            try:\n",
        "                print(f\"episode: {i} - avg reward={result['episode_reward_mean']} - avg episode length={result['episode_len_mean']} - {(time.time() - episode_start)/60} min\")\n",
        "            except:\n",
        "                print(f\"episode: {i} - avg reward={result['episode_reward_mean']} - {(time.time() - episode_start)/60} min\")\n",
        "            episode_start = time.time()\n",
        "        if i%eval_interval==0 and i>0:\n",
        "            eval_trainer(trainer, eval_users, mode=mode, horizon=horizon, target_politics=target_politics, session_length=session_length)\n",
        "\n",
        "    print(f'total time: {(time.time() - train_start)/60}')\n",
        "\n",
        "    t = int(time.time())\n",
        "    trainer.save(f'drive/MyDrive/thesis-training/{agent_name}-{t}/final')\n",
        "    session_histories = ray.get(session_history_tracker.get_session_histories.remote())\n",
        "    with open(f'drive/MyDrive/thesis-training/{agent_name}-{t}/final.pickle', 'wb') as handle:\n",
        "          pickle.dump(session_histories, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    session_histories = ray.get(session_history_tracker.get_session_histories.remote())\n",
        "\n",
        "    plot_results(session_histories=session_histories, window=metrics_window)\n",
        "\n",
        "    eval_trainer(trainer, eval_users, mode=mode, horizon=horizon, target_politics=target_politics, session_length=session_length)\n",
        "    return session_histories"
      ],
      "id": "DF6koS61vhAt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqTABcw2hnkJ"
      },
      "source": [
        "## train"
      ],
      "id": "xqTABcw2hnkJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kttzSVYbMuxS"
      },
      "outputs": [],
      "source": [
        "eval_users = [User().__dict__ for _ in range(1000)]"
      ],
      "id": "kttzSVYbMuxS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTGtHGF4Hpgk"
      },
      "outputs": [],
      "source": [
        "num_workers = 8\n",
        "num_gpus = 1\n",
        "horizon = 100\n",
        "session_length = 500"
      ],
      "id": "nTGtHGF4Hpgk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdyhICCthlZD"
      },
      "source": [
        "### random"
      ],
      "id": "BdyhICCthlZD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6qrsGeS75ZQ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "\n",
        "actor_name = f\"session_history_{time.time()}\"\n",
        "session_history_tracker = SessionHistoryTracker.options(name=actor_name).remote(users=train_users)\n",
        "\n",
        "random_trainer = RandomAgent(\n",
        "  config={\n",
        "      \"env\": get_env(),\n",
        "      \"env_config\": {\n",
        "          \"horizon\": horizon,\n",
        "          \"actor_name\": actor_name,\n",
        "          \"target_politics\": None,\n",
        "          \"mode\": None,\n",
        "          \"eval\": False,\n",
        "          \"session_length\": session_length\n",
        "      },\n",
        "      \"num_workers\": 8,\n",
        "      \"num_gpus\": 1, \n",
        "      \"num_gpus_per_worker\": 1/8, \n",
        "      \"num_envs_per_worker\": 50,\n",
        "  })\n",
        "eval_trainer(random_trainer, eval_users, mode=None, horizon=horizon, target_politics=None, session_length=session_length)"
      ],
      "id": "I6qrsGeS75ZQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48503b1d-cd57-4b9d-bc1a-f5e186bb17ac"
      },
      "source": [
        "### baseline"
      ],
      "id": "48503b1d-cd57-4b9d-bc1a-f5e186bb17ac"
    },
    {
      "cell_type": "code",
      "source": [
        "def test(row):\n",
        "    return (row)"
      ],
      "metadata": {
        "id": "T-Z2Jg2r1XD2"
      },
      "id": "T-Z2Jg2r1XD2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0bQi4GMvrFb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "\n",
        "actor_name = f\"session_history_tracker_{time.time()}\"\n",
        "session_history_tracker = SessionHistoryTracker.options(name=actor_name).remote(users=train_users)\n",
        "\n",
        "baseline_trainer = BaselineAgent(\n",
        "  config={\n",
        "      \"env\": get_env(),\n",
        "      \"env_config\": {\n",
        "          \"horizon\": horizon,\n",
        "          \"actor_name\": actor_name,\n",
        "          \"target_politics\": None,\n",
        "          \"mode\": None,\n",
        "          \"eval\": False\n",
        "      },\n",
        "      \"num_workers\": 8,\n",
        "      \"num_gpus\": 1, \n",
        "      \"num_gpus_per_worker\": 1/8, \n",
        "      \"num_envs_per_worker\": 50,\n",
        "  })\n",
        "\n",
        "eval_trainer(baseline_trainer, eval_users, mode=None, horizon=horizon, target_politics=None, session_length=session_length)"
      ],
      "id": "o0bQi4GMvrFb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9cjEzSvcoZb"
      },
      "outputs": [],
      "source": [
        "x = np.array([[1,2,3], [4,5,6]])\n",
        "sum(x)"
      ],
      "id": "B9cjEzSvcoZb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYTu5zy7cXMp"
      },
      "outputs": [],
      "source": [
        "state = np.array([0,1,2,3,4,5,6,10,10,10,10,10,10,10])\n",
        "print(np.where(np.array(state[:7])<5))\n",
        "# np.argmin(state[:7]) if len(np.where(state[:7])<5)>0 else np.argmax(state[7:])"
      ],
      "id": "TYTu5zy7cXMp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1eU3mc8FOGg"
      },
      "source": [
        "### no manipulation"
      ],
      "id": "G1eU3mc8FOGg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QALai1DeFQvr"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "horizon = 100\n",
        "mode = None\n",
        "target_politics = None\n",
        "actor_name = f\"session_history_tracker_{time.time()}\"\n",
        "session_history_tracker = SessionHistoryTracker.options(name=actor_name).remote()\n",
        "np.random.seed(47)\n",
        "\n",
        "trainer = ApexTrainer(\n",
        "  config={\n",
        "      \"env\": get_env(),\n",
        "      \"env_config\": {\n",
        "          \"horizon\": horizon,\n",
        "          \"actor_name\": actor_name,\n",
        "          \"target_politics\": target_politics,\n",
        "          \"mode\": mode,\n",
        "          \"eval\": False,\n",
        "          \"session_length\": session_length\n",
        "      },\n",
        "      \"num_workers\": num_workers,\n",
        "      \"num_gpus\": num_gpus, \n",
        "      \"num_envs_per_worker\": 100,\n",
        "      \"metrics_smoothing_episodes\": 10000,\n",
        "      \"evaluation_interval\": None,\n",
        "      \"batch_mode\": \"complete_episodes\",\n",
        "      \"seed\": 47,\n",
        "      # \"n_step\": 1,\n",
        "      \"model\": {\n",
        "                'dim': 84,\n",
        "                'fcnet_activation': 'tanh',\n",
        "                'fcnet_hiddens': [256, 256],\n",
        "                'framestack': False,\n",
        "                'use_lstm': False\n",
        "                }\n",
        "  })\n",
        "\n",
        "session_histories = train(n_episodes=300,\n",
        "          session_history_tracker=session_history_tracker,\n",
        "          horizon=horizon, \n",
        "          agent_name=\"no_manipulation\", \n",
        "          eval_users=eval_users,\n",
        "          eval_interval=251, \n",
        "          checkpoint_interval=100,\n",
        "          log_interval=25, \n",
        "          metrics_window=1000, \n",
        "          session_length=session_length,\n",
        "          trainer=trainer)"
      ],
      "id": "QALai1DeFQvr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ebadb1d-ec44-4743-95e2-4e117183835c"
      },
      "source": [
        "### extremes"
      ],
      "id": "6ebadb1d-ec44-4743-95e2-4e117183835c"
    },
    {
      "cell_type": "code",
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "horizon = 100\n",
        "mode = \"manipulate\"\n",
        "target_politics = [0, 6]\n",
        "actor_name = f\"session_history_tracker_{time.time()}\"\n",
        "session_history_tracker = SessionHistoryTracker.options(name=actor_name).remote(users=train_users)\n",
        "np.random.seed(47)\n",
        "\n",
        "config = {\n",
        "      \"env\": get_env(),\n",
        "      \"env_config\": {\n",
        "          \"horizon\": horizon,\n",
        "          \"actor_name\": actor_name,\n",
        "          \"target_politics\": target_politics,\n",
        "          \"mode\": mode,\n",
        "          \"eval\": False,\n",
        "          \"session_length\": session_length\n",
        "      },\n",
        "      \"num_workers\": num_workers,\n",
        "      \"num_gpus\": num_gpus, \n",
        "      \"num_envs_per_worker\": 100,\n",
        "      \"metrics_smoothing_episodes\": 10000,\n",
        "      \"evaluation_interval\": None,\n",
        "      \"batch_mode\": \"complete_episodes\",\n",
        "      \"seed\": 47,\n",
        "      # \"n_step\": 1,\n",
        "      \"model\": {\n",
        "                'dim': 84,\n",
        "                'fcnet_activation': 'tanh',\n",
        "                'fcnet_hiddens': [256, 256],\n",
        "                'framestack': False,\n",
        "                }\n",
        "  }\n",
        "\n",
        "trainer = ApexTrainer(config=config)\n",
        "\n",
        "session_histories = train(n_episodes=4,\n",
        "      session_history_tracker=session_history_tracker,\n",
        "      horizon=horizon, \n",
        "      agent_name=\"extremes\", \n",
        "      eval_users=eval_users,\n",
        "      eval_interval=1,\n",
        "      checkpoint_interval=2,\n",
        "      n=len(train_users), \n",
        "      log_interval=1, \n",
        "      metrics_window=1000, \n",
        "      session_length=session_length,\n",
        "      trainer=trainer)"
      ],
      "metadata": {
        "id": "ay0TTiXUIaPr"
      },
      "id": "ay0TTiXUIaPr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRyrINfJuSod"
      },
      "outputs": [],
      "source": [
        "# session_histories = ray.get(session_history_tracker.get_session_histories.remote())\n",
        "\n",
        "plot_results(session_histories=session_histories, window=1000)\n",
        "\n",
        "eval_trainer(trainer, eval_users, mode=mode, horizon=horizon, target_politics=target_politics, session_length=session_length)"
      ],
      "id": "pRyrINfJuSod"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4c8b787-f8cd-4d00-b0ed-59bb8832797e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "horizon = 100\n",
        "mode = \"manipulate\"\n",
        "target_politics = [0,6]\n",
        "actor_name = f\"session_history_tracker_{time.time()}\"\n",
        "session_history_tracker = SessionHistoryTracker.options(name=actor_name).remote(users=train_users)\n",
        "\n",
        "trainer = ApexTrainer(\n",
        "  config={\n",
        "      \"env\": get_env(),\n",
        "      \"env_config\": {\n",
        "          \"horizon\": horizon,\n",
        "          \"actor_name\": actor_name,\n",
        "          \"target_politics\": target_politics,\n",
        "          \"mode\": mode,\n",
        "          \"eval\": False,\n",
        "          \"session_length\": session_length\n",
        "      },\n",
        "      \"num_workers\": num_workers,\n",
        "      \"num_gpus\": num_gpus, \n",
        "      \"num_envs_per_worker\": 100,\n",
        "      \"metrics_smoothing_episodes\": 10000,\n",
        "      \"evaluation_interval\": None,\n",
        "      # \"soft_horizon\": True,\n",
        "      # \"horizon\": 25\n",
        "  })\n",
        "\n",
        "try:\n",
        "    session_histories = train(n_episodes=500,\n",
        "          session_history_tracker=session_history_tracker,\n",
        "          horizon=horizon, \n",
        "          agent_name=\"extremes\", \n",
        "          eval_users=eval_users,\n",
        "          eval_interval=100,\n",
        "          n=len(train_users), \n",
        "          log_interval=1, \n",
        "          metrics_window=1000, \n",
        "          session_length=session_length,\n",
        "          trainer=trainer)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    session_histories = ray.get(session_history_tracker.get_session_histories.remote())\n",
        "    plot_results(session_histories=session_histories, window=1000)\n",
        "    eval_trainer(trainer, eval_users, mode=mode, horizon=horizon, target_politics=target_politics, session_length=session_length)"
      ],
      "id": "a4c8b787-f8cd-4d00-b0ed-59bb8832797e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "341bda55-c530-4ec0-baec-9ddcff88f3f2"
      },
      "source": [
        "### center"
      ],
      "id": "341bda55-c530-4ec0-baec-9ddcff88f3f2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5Z3qk0LJaN1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "horizon = 100\n",
        "mode = \"manipulate\"\n",
        "target_politics = [2,3,4]\n",
        "actor_name = f\"session_history_{time.time()}\"\n",
        "session_history_tracker = SessionHistoryTracker.options(name=actor_name).remote(users=train_users)\n",
        "np.random.seed(47)\n",
        "\n",
        "trainer = ApexTrainer(\n",
        "  config={\n",
        "      \"env\": get_env(),\n",
        "      \"env_config\": {\n",
        "          \"horizon\": horizon,\n",
        "          \"actor_name\": actor_name,\n",
        "          \"target_politics\": target_politics,\n",
        "          \"mode\": mode,\n",
        "          \"eval\": False,\n",
        "          \"session_length\": session_length\n",
        "      },\n",
        "      \"num_workers\": num_workers,\n",
        "      \"num_gpus\": num_gpus, \n",
        "      \"num_envs_per_worker\": 100,\n",
        "      \"metrics_smoothing_episodes\": 10000,\n",
        "      \"evaluation_interval\": None,\n",
        "      \"batch_mode\": \"complete_episodes\",\n",
        "      \"seed\": 47,\n",
        "      \"model\": {\n",
        "                'dim': 84,\n",
        "                'fcnet_activation': 'tanh',\n",
        "                'fcnet_hiddens': [256, 256],\n",
        "                'framestack': False,\n",
        "                }\n",
        "  })\n",
        "session_histories = train(n_episodes=1000,\n",
        "      session_history_tracker=session_history_tracker,\n",
        "      horizon=horizon, \n",
        "      agent_name=\"center\", \n",
        "      eval_users=eval_users,\n",
        "      eval_interval=1001,\n",
        "      checkpoint_interval=100,\n",
        "      n=len(train_users), \n",
        "      log_interval=25, \n",
        "      metrics_window=1000, \n",
        "      session_length=session_length,\n",
        "      trainer=trainer)"
      ],
      "id": "B5Z3qk0LJaN1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84791c96-aab8-420a-9372-67a8ccc914b6"
      },
      "source": [
        "### right"
      ],
      "id": "84791c96-aab8-420a-9372-67a8ccc914b6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p8E-bUsp6UiE",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "horizon = 100\n",
        "mode = \"manipulate\"\n",
        "target_politics = [4,5,6]\n",
        "actor_name = f\"session_history_{time.time()}\"\n",
        "session_history_tracker = SessionHistoryTracker.options(name=actor_name).remote(users=train_users)\n",
        "np.random.seed(47)\n",
        "\n",
        "trainer = ApexTrainer(\n",
        "  config={\n",
        "      \"env\": get_env(),\n",
        "      \"env_config\": {\n",
        "          \"horizon\": horizon,\n",
        "          \"actor_name\": actor_name,\n",
        "          \"target_politics\": target_politics,\n",
        "          \"mode\": mode,\n",
        "          \"eval\": False,\n",
        "          \"session_length\": session_length\n",
        "      },\n",
        "      \"num_workers\": num_workers,\n",
        "      \"num_gpus\": num_gpus, \n",
        "      \"num_envs_per_worker\": 100,\n",
        "      \"metrics_smoothing_episodes\": 10000,\n",
        "      \"evaluation_interval\": None,\n",
        "      \"batch_mode\": \"complete_episodes\",\n",
        "      \"seed\": 47,\n",
        "      \"n_step\": 1,\n",
        "      \"model\": {\n",
        "                'dim': 84,\n",
        "                'fcnet_activation': 'tanh',\n",
        "                'fcnet_hiddens': [256, 256],\n",
        "                'framestack': False,\n",
        "                }\n",
        "  })\n",
        "try:\n",
        "    session_histories = train(n_episodes=500,\n",
        "          session_history_tracker=session_history_tracker,\n",
        "          horizon=horizon, \n",
        "          agent_name=\"right\", \n",
        "          eval_users=eval_users,\n",
        "          eval_interval=50,\n",
        "          n=len(train_users), \n",
        "          log_interval=25, \n",
        "          metrics_window=1000, \n",
        "          session_length=session_length,\n",
        "          trainer=trainer)\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    session_histories = ray.get(session_history_tracker.get_session_histories.remote())\n",
        "    plot_results(session_histories=session_histories, window=1000)\n",
        "    eval_trainer(trainer, eval_users, mode=mode, horizon=horizon, target_politics=target_politics, session_length=session_length)"
      ],
      "id": "p8E-bUsp6UiE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnjeKQKoyy5v"
      },
      "source": [
        "### neutral"
      ],
      "id": "WnjeKQKoyy5v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma6dMMenbrCH"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "horizon = 100\n",
        "mode = \"manipulate\"\n",
        "target_politics = [3]\n",
        "actor_name = f\"session_history_{time.time()}\"\n",
        "sessionHistory = SessionHistory.options(name=actor_name).remote()\n",
        "\n",
        "trainer = ApexTrainer(\n",
        "  config={\n",
        "      \"env\": get_env(),\n",
        "      \"env_config\": {\n",
        "          \"horizon\": horizon,\n",
        "          \"actor_name\": actor_name,\n",
        "          \"target_politics\": target_politics,\n",
        "          \"mode\": mode,\n",
        "          \"eval\": False\n",
        "      },\n",
        "      \"num_workers\": num_workers,\n",
        "      \"num_gpus\": num_gpus, \n",
        "      \"num_envs_per_worker\": 50,\n",
        "      \"metrics_smoothing_episodes\": 10000,\n",
        "      \"evaluation_interval\": 1000000000,\n",
        "  })\n",
        "\n",
        "\n",
        "session_histories = train(n_episodes=250, \n",
        "      horizon=horizon, \n",
        "      agent_name=\"right\",\n",
        "      eval_users=eval_users, \n",
        "      eval_interval=100, \n",
        "      log_interval=5, \n",
        "      metrics_window=1000, \n",
        "      trainer=trainer)"
      ],
      "id": "Ma6dMMenbrCH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GVy-QgH0pgV"
      },
      "source": [
        "### small reward increase for extreme content"
      ],
      "id": "6GVy-QgH0pgV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AwHjFzp90evE"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()\n",
        "ray.init()\n",
        "horizon = 100\n",
        "mode = \"manipulate\"\n",
        "target_politics = [0, 6]\n",
        "actor_name = f\"session_history_tracker_{time.time()}\"\n",
        "session_history_tracker = SessionHistoryTracker.options(name=actor_name).remote(users=train_users)\n",
        "np.random.seed(47)\n",
        "\n",
        "trainer = ApexTrainer(\n",
        "  config={\n",
        "      \"env\": get_env(),\n",
        "      \"env_config\": {\n",
        "          \"horizon\": horizon,\n",
        "          \"actor_name\": actor_name,\n",
        "          \"target_politics\": target_politics,\n",
        "          \"mode\": mode,\n",
        "          \"eval\": False,\n",
        "          \"session_length\": session_length,\n",
        "          \"target_politics_reward\": 1,\n",
        "          \"other_reward\": 0.9,\n",
        "      },\n",
        "      \"num_workers\": num_workers,\n",
        "      \"num_gpus\": num_gpus, \n",
        "      \"num_envs_per_worker\": 50,\n",
        "      \"metrics_smoothing_episodes\": 10000,\n",
        "      \"evaluation_interval\": None,\n",
        "      \"batch_mode\": \"complete_episodes\",\n",
        "      \"seed\": 47,\n",
        "      # \"n_step\": 1,\n",
        "      \"model\": {\n",
        "                'dim': 84,\n",
        "                'fcnet_activation': 'tanh',\n",
        "                'fcnet_hiddens': [256, 256],\n",
        "                'framestack': False,\n",
        "                }\n",
        "  })\n",
        "\n",
        "# try:\n",
        "session_histories = train(n_episodes=600,\n",
        "      session_history_tracker=session_history_tracker,\n",
        "      horizon=horizon, \n",
        "      agent_name=\"extremes\", \n",
        "      eval_users=eval_users,\n",
        "      eval_interval=50,\n",
        "      n=len(train_users), \n",
        "      log_interval=25, \n",
        "      metrics_window=1000, \n",
        "      session_length=session_length,\n",
        "      trainer=trainer)\n",
        "# except Exception as e:\n",
        "#     print(e)\n",
        "#     session_histories = ray.get(session_history_tracker.get_session_histories.remote())\n",
        "#     plot_results(session_histories=session_histories, window=1000)\n",
        "#     eval_trainer(trainer, eval_users, mode=mode, horizon=horizon, target_politics=target_politics, session_length=session_length)\n"
      ],
      "id": "AwHjFzp90evE"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "9qhToUZA2kPt",
        "q2GooRYrb5As",
        "E_aVpBKRZ03P"
      ],
      "machine_shape": "hm",
      "name": "PODSNPs.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-7.m87",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}